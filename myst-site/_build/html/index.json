{"kind":"Article","sha256":"1a9534523162253774cfe9e14a06dfb013a8a6096ca8038afdf47d55a1c90182","slug":"index","location":"/index.md","dependencies":[],"frontmatter":{"title":"Machine Learning Guide","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Alex Sciuto","given":"Alex","family":"Sciuto"},"name":"Alex Sciuto","affiliations":["University of Central Florida"],"id":"contributors-myst-generated-uid-0"}],"affiliations":[{"id":"University of Central Florida","name":"University of Central Florida"}],"exports":[{"format":"md","filename":"index.md","url":"/build/index-6b305f3637ed8573b48d9b9d3919b702.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Welcome to the Machine Learning Guide! This resource is designed to help you understand machine learning concepts, from basic models to advanced neural networks and natural language processing (NLP) tasks. This guide follows an Occam’s razor approach, starting with simpler models and gradually progressing to more complex ones. Each topic includes theoretical explanations, code examples, and data visualizations.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"lI6h6CWDWQ"}],"key":"BoF50UnFCt"},{"type":"heading","depth":2,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Table of Contents","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"FgeQgmBBy6"}],"identifier":"table-of-contents","label":"Table of Contents","html_id":"table-of-contents","implicit":true,"key":"Yzqd1hVBhy"},{"type":"heading","depth":3,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"1. Introduction to Machine Learning","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"m8DQloowqw"}],"identifier":"id-1-introduction-to-machine-learning","label":"1. Introduction to Machine Learning","html_id":"id-1-introduction-to-machine-learning","implicit":true,"key":"TwpKTHtGfr"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":12,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"link","url":"/intro-to-ml","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"strong","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"What is Machine Learning?","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"fHFwQB8ztD"}],"key":"w4qBbT9hE9"}],"urlSource":"intro/01-intro-to-ml.ipynb","dataUrl":"/intro-to-ml.json","internal":true,"protocol":"file","key":"gDxdzUiO2z"},{"type":"text","value":": Overview of machine learning concepts, definitions, and applications.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Aw5wBU0x0y"}],"key":"nyRuOiwL4U"},{"type":"listItem","spread":true,"position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"link","url":"/types-of-ml","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"strong","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Types of Machine Learning","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"wW6RfEJ7C9"}],"key":"ihiGwb1xWc"}],"urlSource":"intro/02-types-of-ml.ipynb","dataUrl":"/types-of-ml.json","internal":true,"protocol":"file","key":"kfEKkXK8pq"},{"type":"text","value":": Supervised, unsupervised, semi-supervised, and reinforcement learning.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"HZ7NWwyrl6"}],"key":"FmuZ5R6grF"},{"type":"listItem","spread":true,"position":{"start":{"line":14,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"link","url":"/build/03-occams-razor-b75921ac9b7cb220d20a989a42ee330c.ipynb","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"strong","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Occam’s Razor in Machine Learning","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"YzuzaBoc02"}],"key":"STlhcKGeL7"}],"urlSource":"intro/03-occams-razor.ipynb","static":true,"protocol":"file","key":"uc0vpEyATC"},{"type":"text","value":": Importance of simplicity in model selection and avoiding overfitting.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"iEmNslqPQG"}],"key":"dCfTN1Jsvs"}],"key":"use2H1Sc97"},{"type":"heading","depth":3,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"2. Basic Statistics and Data Exploration","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"T172zgKjlJ"}],"identifier":"id-2-basic-statistics-and-data-exploration","label":"2. Basic Statistics and Data Exploration","html_id":"id-2-basic-statistics-and-data-exploration","implicit":true,"key":"RlesKHbunU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":17,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"link","url":"/build/01-descriptive-stati-ebc0f41aab8f20064b0686a9862d17c8.ipynb","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"strong","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Descriptive Statistics","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"PEG2LqiBET"}],"key":"YtC1Epi1iC"}],"urlSource":"stats/01-descriptive-statistics.ipynb","static":true,"protocol":"file","key":"aRW1IY8kfJ"},{"type":"text","value":": Mean, median, mode, variance, standard deviation. Data visualization: histograms, box plots, and scatter plots.","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"Mccpj7wHIS"}],"key":"hXRabarAKB"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"link","url":"/build/02-data-distribution-ab53876e242fdb94b94f9f456ce8819c.ipynb","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Data Distribution","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"mNHn02i8en"}],"key":"r9cEcKwcI4"}],"urlSource":"stats/02-data-distribution.ipynb","static":true,"protocol":"file","key":"MzM1iFGAFu"},{"type":"text","value":": Understanding normal distribution, skewness, kurtosis. Visualizing data distribution with histograms and Q-Q plots.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"yX5hGeSNuv"}],"key":"eSLbD2C0aV"},{"type":"listItem","spread":true,"position":{"start":{"line":19,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"link","url":"/build/03-correlation-covar-719e3f948e9a380b6b5c715f4f5ef363.ipynb","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Correlation and Covariance","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"CfQVAXjhR0"}],"key":"Z4h8PImM6Z"}],"urlSource":"stats/03-correlation-covariance.ipynb","static":true,"protocol":"file","key":"WOlruDUI7c"},{"type":"text","value":": Understanding relationships between variables. Visualizing relationships with heatmaps and pair plots.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"ZDjKbkkUt9"}],"key":"gKTw6isuTl"}],"key":"l9OiU4y4ua"},{"type":"heading","depth":3,"position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"3. Data Preprocessing and Feature Engineering","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"yMxaX1nzWV"}],"identifier":"id-3-data-preprocessing-and-feature-engineering","label":"3. Data Preprocessing and Feature Engineering","html_id":"id-3-data-preprocessing-and-feature-engineering","implicit":true,"key":"HTAXWfV2ER"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":22,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"link","url":"/build/01-missing-values-eeddf5926fb41cd5c4370beb31a14ae4.ipynb","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"Handling Missing Values","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"SLsHUQADZQ"}],"key":"CAwoIKU8i4"}],"urlSource":"preprocessing/01-missing-values.ipynb","static":true,"protocol":"file","key":"zR4VtfwKh5"},{"type":"text","value":": Techniques such as mean/mode imputation, KNN imputation.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"qMiIYmFLfH"}],"key":"OuxHqrCh4m"},{"type":"listItem","spread":true,"position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"link","url":"/build/02-feature-scaling-ffe5d4e52ccd4e2791c4429688504183.ipynb","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"strong","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"Feature Scaling and Normalization","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"lN1ASd4xCP"}],"key":"eifaGNMEW9"}],"urlSource":"preprocessing/02-feature-scaling.ipynb","static":true,"protocol":"file","key":"qw2LVOovqw"},{"type":"text","value":": Standardization, min-max scaling, and log transformation.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"yLPwCqkQhM"}],"key":"zGtifw57hZ"},{"type":"listItem","spread":true,"position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"link","url":"/build/03-categorical-encod-7b3c7284262ccafe75f447928cc64c78.ipynb","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"strong","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"children":[{"type":"text","value":"Categorical Data Encoding","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"j2k37nbKdv"}],"key":"vXVr9hiZWd"}],"urlSource":"preprocessing/03-categorical-encoding.ipynb","static":true,"protocol":"file","key":"C1pbiUSGtb"},{"type":"text","value":": One-hot encoding, label encoding, and target encoding.","position":{"start":{"line":24,"column":1},"end":{"line":24,"column":1}},"key":"yhHZjjwCdv"}],"key":"DJ8uKFvmGV"},{"type":"listItem","spread":true,"position":{"start":{"line":25,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"link","url":"/build/04-feature-selection-cabf8d94ca4ad6c12af2a32e9b8d875e.ipynb","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"strong","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"Feature Selection Techniques","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"w7MPQiEpqc"}],"key":"er51HjWyx7"}],"urlSource":"preprocessing/04-feature-selection.ipynb","static":true,"protocol":"file","key":"vbdXXWN0GV"},{"type":"text","value":": Techniques like correlation analysis, chi-square test, and LASSO for selecting relevant features.","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"tjA4giNc5I"}],"key":"BfG9ODNe0q"}],"key":"yfaUKms3h8"},{"type":"heading","depth":3,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"4. Simple Models and Baselines","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"T0EfDFOdwk"}],"identifier":"id-4-simple-models-and-baselines","label":"4. Simple Models and Baselines","html_id":"id-4-simple-models-and-baselines","implicit":true,"key":"K34R71Y7DW"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":28,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"link","url":"/build/01-mean-median-769bfea6e5cafcc0ab691211ba670ce6.ipynb","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"strong","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Mean and Median as Predictors","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"H7zigrKc4s"}],"key":"Tje2o9TJZM"}],"urlSource":"simple/01-mean-median.ipynb","static":true,"protocol":"file","key":"Rlwm08eiLe"},{"type":"text","value":": Using simple baselines for regression and classification tasks.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"oyO09xoSTY"}],"key":"KeadgasDyj"},{"type":"listItem","spread":true,"position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"link","url":"/build/02-linear-regression-23c0e2806e5ba718c989d6c398128f12.ipynb","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"strong","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"Linear Regression","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"AjqhzLStU6"}],"key":"rAlXtqqmXW"}],"urlSource":"simple/02-linear-regression.ipynb","static":true,"protocol":"file","key":"rkz0xtJVTL"},{"type":"text","value":": Theory, assumptions, and interpretation. Includes predicting house prices with visualization of residual plots and coefficients.","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"sEdb79GuUj"}],"key":"KEIqK3uXB6"},{"type":"listItem","spread":true,"position":{"start":{"line":30,"column":1},"end":{"line":31,"column":1}},"children":[{"type":"link","url":"/build/03-logistic-regressi-fee8aefe30df8a2102e985817fb408f3.ipynb","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"strong","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"Logistic Regression","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"CFHdfPgx81"}],"key":"MtiquOZdKu"}],"urlSource":"simple/03-logistic-regression.ipynb","static":true,"protocol":"file","key":"ZjbFvqNMz7"},{"type":"text","value":": Theory of logistic regression, sigmoid function, and decision boundary. Titanic dataset classification project with ROC curves and confusion matrices.","position":{"start":{"line":30,"column":1},"end":{"line":30,"column":1}},"key":"vEixDulhC9"}],"key":"vlz7VPsgYz"}],"key":"g4eHt0flyC"},{"type":"heading","depth":3,"position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"children":[{"type":"text","value":"5. Decision Trees and Ensemble Methods","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"c0LkHbfBEc"}],"identifier":"id-5-decision-trees-and-ensemble-methods","label":"5. Decision Trees and Ensemble Methods","html_id":"id-5-decision-trees-and-ensemble-methods","implicit":true,"key":"Rve86Sxs0V"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":33,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"link","url":"/build/01-decision-trees-723636962f69b6b6ab62059ecbf0ba51.ipynb","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"strong","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"children":[{"type":"text","value":"Decision Trees","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"vMRQz0uhpz"}],"key":"KqhM0cSaxU"}],"urlSource":"ensemble/01-decision-trees.ipynb","static":true,"protocol":"file","key":"KHOPGR4Ndh"},{"type":"text","value":": Theory of decision trees, including Gini impurity and information gain. Classifying the Iris dataset with visualization of decision tree plots.","position":{"start":{"line":33,"column":1},"end":{"line":33,"column":1}},"key":"mdR4vjHqI1"}],"key":"Vwm9Hfg60H"},{"type":"listItem","spread":true,"position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"link","url":"/build/02-random-forests-f943ac042c4bf8ca0a075773cceb7784.ipynb","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"strong","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"children":[{"type":"text","value":"Random Forests","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"o07gB3yOA8"}],"key":"JlMQvr00Dl"}],"urlSource":"ensemble/02-random-forests.ipynb","static":true,"protocol":"file","key":"U0Mqi5IvOA"},{"type":"text","value":": Bagging, out-of-bag error, and feature importance. Classifying wine quality with feature importance and error plots.","position":{"start":{"line":34,"column":1},"end":{"line":34,"column":1}},"key":"AEncuxa4QS"}],"key":"F9U3d86qpQ"},{"type":"listItem","spread":true,"position":{"start":{"line":35,"column":1},"end":{"line":36,"column":1}},"children":[{"type":"link","url":"/build/03-gradient-boosting-76ea7a8ab3fbe02496a5d60aa5344916.ipynb","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"strong","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"Gradient Boosting and XGBoost","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"oLDmxjy5AV"}],"key":"XadOcQ9c7z"}],"urlSource":"ensemble/03-gradient-boosting.ipynb","static":true,"protocol":"file","key":"CTqTHyltId"},{"type":"text","value":": Boosting techniques, learning rate, and regularization. Predicting Boston housing prices with SHAP plots and feature interactions.","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"wxwy0IBLff"}],"key":"HyHIPpAPoV"}],"key":"w4HLjbV5tm"},{"type":"heading","depth":3,"position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"6. Support Vector Machines (SVM)","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"f4cyIklWj1"}],"identifier":"id-6-support-vector-machines-svm","label":"6. Support Vector Machines (SVM)","html_id":"id-6-support-vector-machines-svm","implicit":true,"key":"p16BPfUL8X"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":38,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"link","url":"/build/01-linear-svm-43627f596adc821c28237e3d5e9a4ded.ipynb","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"strong","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Linear SVM","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"UWHIy4FBQu"}],"key":"jdyPR4wjqs"}],"urlSource":"svm/01-linear-svm.ipynb","static":true,"protocol":"file","key":"PbOm3A01bM"},{"type":"text","value":": Understanding hyperplanes and margin maximization. Binary classification with visualization of decision boundaries and support vectors.","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"jhBu8bwUBO"}],"key":"J8eHfAK9Wo"},{"type":"listItem","spread":true,"position":{"start":{"line":39,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"link","url":"/build/02-kernel-svm-df79e19b6506944c4f422236d007bb52.ipynb","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"strong","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"children":[{"type":"text","value":"Kernel SVM","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"fG3f3rw5uf"}],"key":"gkIGVdildH"}],"urlSource":"svm/02-kernel-svm.ipynb","static":true,"protocol":"file","key":"Gc1EGQpEUH"},{"type":"text","value":": Theory of kernel functions for non-linear decision boundaries. Classifying non-linear data with different kernel functions.","position":{"start":{"line":39,"column":1},"end":{"line":39,"column":1}},"key":"LvYlg0LQAx"}],"key":"r5qB209euE"}],"key":"oaGXe5tHGb"},{"type":"heading","depth":3,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"7. Clustering and Dimensionality Reduction","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"SWuIQWx1dr"}],"identifier":"id-7-clustering-and-dimensionality-reduction","label":"7. Clustering and Dimensionality Reduction","html_id":"id-7-clustering-and-dimensionality-reduction","implicit":true,"key":"xiFnAT3Rr5"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":42,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"link","url":"/k-means","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"strong","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"K-Means Clustering","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"gvY2jflJx3"}],"key":"GQLCq9MATN"}],"urlSource":"clustering/01-k-means.ipynb","dataUrl":"/k-means.json","internal":true,"protocol":"file","key":"WXWnl15Nn3"},{"type":"text","value":": Centroid-based clustering, inertia, and the elbow method. Customer segmentation with cluster plots and silhouette score.","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"Pn5OwG3Ur9"}],"key":"CsHQ30wTF2"},{"type":"listItem","spread":true,"position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"link","url":"/build/02-hierarchical-clus-851f68ba25ebe2936cd633a936e89d39.ipynb","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"strong","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"Hierarchical Clustering","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"ZdmqoJOiPp"}],"key":"caYIgb6VQh"}],"urlSource":"clustering/02-hierarchical-clustering.ipynb","static":true,"protocol":"file","key":"WOF02idWH1"},{"type":"text","value":": Theory of dendrograms and linkage methods. Visualizing hierarchical clusters with dendrogram plots and heatmaps.","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"ZwRT4aS670"}],"key":"rbionJUzIB"},{"type":"listItem","spread":true,"position":{"start":{"line":44,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"link","url":"/build/03-pca-a59c0410d141655fc32cc86fd51afd41.ipynb","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"strong","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"children":[{"type":"text","value":"Principal Component Analysis (PCA)","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"EYrw5oBaQ6"}],"key":"Pw83gFSsGo"}],"urlSource":"clustering/03-pca.ipynb","static":true,"protocol":"file","key":"q6ZjSZmcVe"},{"type":"text","value":": Dimensionality reduction and variance explanation. Visualizing high-dimensional data with explained variance plots and PCA plots.","position":{"start":{"line":44,"column":1},"end":{"line":44,"column":1}},"key":"s0PIulENo2"}],"key":"MtQIKSUFvq"}],"key":"LgoWlHUuQB"},{"type":"heading","depth":3,"position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"children":[{"type":"text","value":"8. Advanced Regression Techniques","position":{"start":{"line":46,"column":1},"end":{"line":46,"column":1}},"key":"ZEJAxbQvGa"}],"identifier":"id-8-advanced-regression-techniques","label":"8. Advanced Regression Techniques","html_id":"id-8-advanced-regression-techniques","implicit":true,"key":"tYNyrw4C45"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":47,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"link","url":"/build/01-ridge-lasso-d412c22518c26e56a54a89f51081c514.ipynb","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"strong","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"Ridge and Lasso Regression","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"VvjGs1CLVa"}],"key":"Vdn2Sdfs2W"}],"urlSource":"advanced-regression/01-ridge-lasso.ipynb","static":true,"protocol":"file","key":"lYrciVVrSj"},{"type":"text","value":": Regularization techniques and bias-variance tradeoff. Predicting real estate prices with coefficient paths and error plots.","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"izfn5uZMCS"}],"key":"G6nPfGfLrT"},{"type":"listItem","spread":true,"position":{"start":{"line":48,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"link","url":"/build/02-polynomial-932ce031a7f75ce5272f38d502420ed7.ipynb","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"strong","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Polynomial Regression","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"Dw284KPWYd"}],"key":"UY7IZblL5R"}],"urlSource":"advanced-regression/02-polynomial.ipynb","static":true,"protocol":"file","key":"alWkNdLOhB"},{"type":"text","value":": Modeling non-linear relationships. Visualization of curve fitting and residual plots.","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"mlhOr6T0Rk"}],"key":"xkklIfBEPy"}],"key":"dkFQCPkCyt"},{"type":"heading","depth":3,"position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"text","value":"9. Neural Networks and Deep Learning","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"zooManpJoV"}],"identifier":"id-9-neural-networks-and-deep-learning","label":"9. Neural Networks and Deep Learning","html_id":"id-9-neural-networks-and-deep-learning","implicit":true,"key":"w45WEqTsGj"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":51,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"link","url":"/build/01-mlp-e4d2146164a4099692499cc114baceda.ipynb","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"strong","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"children":[{"type":"text","value":"Multilayer Perceptron (MLP)","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"KCHQ2rzImc"}],"key":"hW6F2mZM85"}],"urlSource":"deep-learning/01-mlp.ipynb","static":true,"protocol":"file","key":"XeuWiCwTHV"},{"type":"text","value":": Understanding perceptrons, activation functions, and backpropagation. Handwritten digit classification (MNIST) with loss curves and accuracy plots.","position":{"start":{"line":51,"column":1},"end":{"line":51,"column":1}},"key":"QBUdwf6gVm"}],"key":"KGmR9qsjA7"},{"type":"listItem","spread":true,"position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"link","url":"/build/02-cnn-52f6605636a8bb0fb335c0f6cabbdccb.ipynb","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"strong","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"Convolutional Neural Networks (CNN)","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"cmxDlVKGcQ"}],"key":"xnRdiWHuWb"}],"urlSource":"deep-learning/02-cnn.ipynb","static":true,"protocol":"file","key":"xswW47j0tU"},{"type":"text","value":": Convolutional layers, pooling, and feature maps. Image classification (CIFAR-10) with feature map visualization and filter visualization.","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"oPas9emB2o"}],"key":"MpSZI2IRpB"},{"type":"listItem","spread":true,"position":{"start":{"line":53,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"link","url":"/build/03-rnn-lstm-f24ebaf8966cf7493171bcbf4cc21eeb.ipynb","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"strong","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"children":[{"type":"text","value":"Recurrent Neural Networks (RNN) and LSTMs","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"OouOo45JSn"}],"key":"wgSRxm4RbD"}],"urlSource":"deep-learning/03-rnn-lstm.ipynb","static":true,"protocol":"file","key":"CgpfqCN094"},{"type":"text","value":": Sequence modeling, vanishing gradient problem, and LSTMs. Sentiment analysis on text data with RNN activations and attention plots.","position":{"start":{"line":53,"column":1},"end":{"line":53,"column":1}},"key":"kCxZhF78Uj"}],"key":"fSYIBVlFJf"}],"key":"ffvOU0S4fn"},{"type":"heading","depth":3,"position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"10. Natural Language Processing (NLP)","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"MEHegrI99D"}],"identifier":"id-10-natural-language-processing-nlp","label":"10. Natural Language Processing (NLP)","html_id":"id-10-natural-language-processing-nlp","implicit":true,"key":"ZttupUBrWa"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":56,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"link","url":"/build/01-text-preprocessin-b34ee5edbec5036f74574e8451afdb71.ipynb","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"strong","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"children":[{"type":"text","value":"Text Preprocessing","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"SoYa2penOW"}],"key":"o0QzREaqh0"}],"urlSource":"nlp/01-text-preprocessing.ipynb","static":true,"protocol":"file","key":"RakTbWNy65"},{"type":"text","value":": Techniques like tokenization, stemming, lemmatization, and stop-word removal.","position":{"start":{"line":56,"column":1},"end":{"line":56,"column":1}},"key":"YC1QkMNXjd"}],"key":"Y6j5wLY0sI"},{"type":"listItem","spread":true,"position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"link","url":"/build/02-word-embeddings-6400309d2a5e078675e90b57b54c946b.ipynb","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"strong","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"Word Embeddings","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"RYRH60PkLy"}],"key":"uqJeavnUFk"}],"urlSource":"nlp/02-word-embeddings.ipynb","static":true,"protocol":"file","key":"RChRSfYqtm"},{"type":"text","value":": Understanding Word2Vec, GloVe, and contextual embeddings. Word similarity and analogy tasks.","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"wuE3koj2yA"}],"key":"gGyW9aibff"},{"type":"listItem","spread":true,"position":{"start":{"line":58,"column":1},"end":{"line":59,"column":1}},"children":[{"type":"link","url":"/build/03-text-classificati-7494801a8a1fc17b4b70b33dc747c6dd.ipynb","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"strong","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"children":[{"type":"text","value":"Text Classification and Sentiment Analysis","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"AK2ssN0Q20"}],"key":"xBQTHhAUVD"}],"urlSource":"nlp/03-text-classification.ipynb","static":true,"protocol":"file","key":"kd6s7NTi9q"},{"type":"text","value":": Bag-of-words, TF-IDF, and LSTM for text classification. Sentiment analysis on movie reviews with word clouds and confusion matrices.","position":{"start":{"line":58,"column":1},"end":{"line":58,"column":1}},"key":"Xmdn6SeY4R"}],"key":"SEIdpuhtsz"}],"key":"xzt0kqTswG"},{"type":"heading","depth":3,"position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"11. Advanced Topics and Special Cases","position":{"start":{"line":60,"column":1},"end":{"line":60,"column":1}},"key":"j9LJuHsTba"}],"identifier":"id-11-advanced-topics-and-special-cases","label":"11. Advanced Topics and Special Cases","html_id":"id-11-advanced-topics-and-special-cases","implicit":true,"key":"MvHqHWQz4U"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":61,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"link","url":"/build/01-gans-f53fb779fad9aecae1b029debbeb4e75.ipynb","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"strong","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"children":[{"type":"text","value":"Generative Adversarial Networks (GANs)","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"GTZIroVnJA"}],"key":"Odghgu2UJR"}],"urlSource":"advanced/01-gans.ipynb","static":true,"protocol":"file","key":"hzcSCWgc4K"},{"type":"text","value":": Theory of GANs, generator and discriminator models. Image generation on MNIST dataset with visualization of generated images.","position":{"start":{"line":61,"column":1},"end":{"line":61,"column":1}},"key":"hGZtxCxIPp"}],"key":"ftc4vwBBL0"},{"type":"listItem","spread":true,"position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"link","url":"/build/02-transfer-learning-8627784b6a077520e6f757ca05785e05.ipynb","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"strong","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"children":[{"type":"text","value":"Transfer Learning","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"iRdyuvuGZe"}],"key":"Bh5t087FrL"}],"urlSource":"advanced/02-transfer-learning.ipynb","static":true,"protocol":"file","key":"o3WEUmsZDi"},{"type":"text","value":": Using pre-trained models and fine-tuning. Image classification with ResNet and visualization of feature maps.","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"yr1Ssh62Lk"}],"key":"tFNipBrwBv"},{"type":"listItem","spread":true,"position":{"start":{"line":63,"column":1},"end":{"line":64,"column":1}},"children":[{"type":"link","url":"/build/03-reinforcement-lea-8aaad34027b1bb180e4de794668d2021.ipynb","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"strong","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Reinforcement Learning","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"WbKjy4v3Fb"}],"key":"hF5OmIDV2L"}],"urlSource":"advanced/03-reinforcement-learning.ipynb","static":true,"protocol":"file","key":"sCcXZ4UJLY"},{"type":"text","value":": Markov Decision Processes, Q-learning, and policy gradients. Solving OpenAI Gym environments with reward plots and action distributions.","position":{"start":{"line":63,"column":1},"end":{"line":63,"column":1}},"key":"nmRM4RmaPW"}],"key":"yIo3OqXOYn"}],"key":"hCK4wEBcq1"},{"type":"heading","depth":3,"position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"12. Model Evaluation and Interpretability","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"fKjJK3hEg6"}],"identifier":"id-12-model-evaluation-and-interpretability","label":"12. Model Evaluation and Interpretability","html_id":"id-12-model-evaluation-and-interpretability","implicit":true,"key":"qsBtiV9OKw"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":66,"column":1},"end":{"line":69,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"link","url":"/build/01-evaluation-metric-fc9ad24737820318766e0bf88dd9c89c.ipynb","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"strong","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"children":[{"type":"text","value":"Model Evaluation Metrics","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"WubO4GKYSA"}],"key":"wg5vWHv9K3"}],"urlSource":"evaluation/01-evaluation-metrics.ipynb","static":true,"protocol":"file","key":"VLCCw2DkNE"},{"type":"text","value":": Metrics like accuracy, precision, recall, F1 score, ROC-AUC, MAE, and RMSE.","position":{"start":{"line":66,"column":1},"end":{"line":66,"column":1}},"key":"UO1miVWmTy"}],"key":"TfFvoM5Swy"},{"type":"listItem","spread":true,"position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"link","url":"/build/02-cross-validation-b51897015dc5285a4881710b5ecd949e.ipynb","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"strong","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"children":[{"type":"text","value":"Cross-Validation and Hyperparameter Tuning","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"YGAhvxab50"}],"key":"QymTMPDelX"}],"urlSource":"evaluation/02-cross-validation.ipynb","static":true,"protocol":"file","key":"VeZzoB5X32"},{"type":"text","value":": k-fold cross-validation, grid search, and random search. Hyperparameter tuning for Random Forests with validation curves and grid search results.","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"YeuPxdLuPA"}],"key":"dA9ydwcvvJ"},{"type":"listItem","spread":true,"position":{"start":{"line":68,"column":1},"end":{"line":69,"column":1}},"children":[{"type":"link","url":"/build/03-model-interpretab-03ca5696386d9a34aa6a8798d0e0e82d.ipynb","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"strong","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"Model Interpretability Techniques","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"SUQgM8BNIR"}],"key":"qcdWERpuDD"}],"urlSource":"evaluation/03-model-interpretability.ipynb","static":true,"protocol":"file","key":"eUSqyKMEGd"},{"type":"text","value":": LIME, SHAP, and partial dependence plots. Interpret a complex model (XGBoost) on the Titanic dataset with SHAP plots and feature importance.","position":{"start":{"line":68,"column":1},"end":{"line":68,"column":1}},"key":"cVXIF64Gc4"}],"key":"RNCQleC9Kw"}],"key":"MyC5t8Uac6"},{"type":"thematicBreak","position":{"start":{"line":70,"column":1},"end":{"line":70,"column":1}},"key":"yaUMk3FfZA"},{"type":"paragraph","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"children":[{"type":"strong","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"children":[{"type":"text","value":"Explore the guide by navigating through the sections above.","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"key":"txTo8wVDhD"}],"key":"NQasipMG1u"},{"type":"text","value":" Each topic contains both theoretical explanations and practical code examples to help you get hands-on experience with machine learning concepts.","position":{"start":{"line":72,"column":1},"end":{"line":72,"column":1}},"key":"ul5Knz1dQW"}],"key":"tvZhxXHIcv"},{"type":"thematicBreak","position":{"start":{"line":74,"column":1},"end":{"line":74,"column":1}},"key":"ByIfhDOdvx"}],"key":"ebhwfUy6AR"}],"key":"wtoJVxBdPj"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"next":{"title":"intro-to-ml","url":"/intro-to-ml","group":"Introduction"}}},"domain":"http://localhost:3006"}